# Сессия Q&A

## Встроенный дашборд кликхауса.

Дашборд представляет собой просто веб-страничку. Кликхаус отдает собой просто хардкоженную html, вебклиент отправляет запросы в кликхаус и благодаря js-кам отрисовывает графики. Запросы для графиков берутся при помощи запроса `select title, query from system.dashboards where dashboard='Overview';`, запрос похож на параметризированную вьюшку. Также мы можем выполнить запрос в браузере `hostname:8123/play`. Мы можем создать собственную таблицу со структурой аналогичной `system.dashboards` , чтобы забирать title и query, в query должно подставляться rounding, seconds. При этом все изменения в запросы в UI - все урл-энкодится (то есть урл поменяется и сохранится! можно будет добавит его в закладки)

## Проекции

МВ занимаются только перекладыванием данных (причем не только в таблицы в кх, но и в кафку, мускул, постгрю). Это что то на уровне ЕТЛ процесса (перекладывание с последующими трансформациями, самостоятельные данные). У проекций функционал победнее, это зависимые данные (данные, которые формируются на основе имеющихся). Можно считать что проекция - это еще один первичный индекс. Проекции не дают такого эффекта как держать рядом полную копию данных, но отсортированных по другому ключу. Если у нас таблица партиционированная - то у нас сначала формируются парты по нашему основному ключу, затем в пределах этих партов - сортируем еще по одному ключу. То есть изначально мы все равно по `max_insert_block_size` нарезать по первичному ключу и в идеале после того как они досхлопнутся до больших 100гб-тных партов , когда пройдут все фоновые процессы по оптимизации этих данных (мерджи), - тогда можно эффективно построить вокруг этого еще проекцию, сожрав еще 100гб, но заимев внутри копию данных с другим ключом, таким образом значительно ускорив. Если же парты маленькие (optimize внутренний еще не прошел) - то рекомендуется смотреть в сторону семейства метрик SchedulePool (наши пулы по объединению данных в большие куски), возможно стоит дать больше тредов, тогда проекция еще больше разгонится.

## Репликация

Макросы можно посмотреть в табличке `system.macros`.

По поводу взаимодействия зукипера и кх. При вставке (`insert into replicated_table1 ('data');`) кликхаус принимает данные в том формате, в котором мы ему их подали, на парсер. Парсер кх рожает под собой блоки, с блоков рождаются парты, при рождении парта перед тем как он появится в кликхаусе в system.parts делается обязательно запись в зукипер и в system.replication_queue (там появляется событие GET_PART - для текущей реплики оно уже заранее считается выполненным, потому что мы именно в эту реплику вставили). По событию GET_PART (поскольку оно в очередь репликации в зукипере пишется, replication_queue берет данные оттуда) другие реплики видят это событие и их очереди репликации по имени парта идут на интерсервер порт кликзауса и по хттп возьмут данные с той репллики, на которой этот парт родился. При мердже создается событие MERGE_PARTS и у клика есть ручка (что то вроде prefer_to_fetch_..._merge), если она включена, то клик будет перевыкачивать смердженные соседние реплики, жертвуюя сетью в угоду экономии процессора (иначе будет повторять ту же самую операцию над теми же самыми партами). Еще есть событие REPLACE_RANGE - это то, что появляется, когда мы говорим ALTER TABLE REPLACE PARTITION из какой то еще партиции (attach выгляит просто как гет). 

В кликхаусе есть множество интеграционных таблиц. Это таблицы в которых кликхаус работает с какой то внешней системой (под ними данных нет, есть только какие то метаданные). Кроме этого есть интеграционные движки - с ними мы получаем комплект созданных таблиц в базе system, обычно движок у них говорящий. Так, если мы выполним запрос `show create table system.replication_queue`, мы увидим `Engine=SystemReplicationQueue`. Они позволяют заглянуть в голову кликхауса либо во внешнюю таблицу. Полный список таких движков можем увидеть в результате запроса `select name, engine from system.tables where database = 'system';`

Если одна из реплик лежит - то зукипер хранит эту информацию и затем отдаст все то, что должно быть реплицировано, в system.replication_queue (в зукипере табличка несколько более полная). Реплика лежит - но в зукипере табличка продолжает писаться, но позиция реплики не инкрементируется! За это отвечает log_pointer (по нему она выгрузит из system.replication_queue все, что больше этого log_pointer). Сама очередь хранится в зукипере. При этом елси в очереди репликации есть дроп таблицы - клик может отбросить лишние записи в очереди репликации.

## Денормализованные витрины

Условный постгрес стоит испльзовать до тех пор пока нас устраивает производительность запросов на нашем количестве данных. Как только какие то аналитические запросы начинают выполняться в пг десятки минут, в то время как в клике они выполняются доли секунд - тогда и стоит переходить. Конкретного порога нет. Как правило задумываются о переходе когда становится больше десятка-другого серверов.

Лучше всего переделывать сторонним етл-процессом, который будет постепенно обкачивать наш постгрес, выполняя джоины порционно (атомарными пачками). Тем самым походу переливки пропушивать в кликхаус. То есть етл процесс лучше выстраивать где то сбоку. Если это дорого по разработке и часам - то в клике есть движок постгрес, но нужно понимать, что клик не интерпретирует запросы кх в запросы пг (допустим, лимит не интерпретируется! сначала сделается фулскан таблицы, передастся по сети, интерпретируется в понятные форматы - и только потом сделается лимит). 
