# Репликация и другие фоновые процессы

https://chistadata.com/data-replication-in-clickhouse-docker-based-setup/

## Виды репликации в разных системах

С репликацией можно было встречаться еще в постгресе. С точки зрения репликации выделяют 2 варианта:
- синхронная - запрос считается выполненным тогда, когда он выполнен на всех репликах, как правило, все реплики лидеры. Дополнительные механизмы синхронизации выполнения запроса на репликах (от примитичных - условный health-check на передачу информации, - до более продвинутых).
- асинхронная - запрос считается выполненым тогда, когда он выполнен на принимающей запрос реплике. Репликация запроса выполняется независимо на все реплики, в фоновом режиме. Как правило, один лидер, остальные реплики. Однако мульти-лидер также возможен с дополнительными механизамами обеспечения консистентности. Тут мы отзыва не ждем - просто запускаем фоновые процессы, которые будут рассылать сигналы, процессы ничего не блокируют. Тут есть такой принцип как split-brain (достаточно плохая штука, тк может привести к несогласованности данных).

В зависимости от того, как каждая из них происходит, ее применяют в разных юз-кейсах. Условно говоря про какой нибудь большой распределенный постгрес (гринплам) мы будем говорить про систему, которая испльзует mirror-link (зеркалирование) по принципу физической репликации. То есть по сути передача файлов изменений (на постгресе это называется write-ahead логи) с одной ноды на другую ноду. При этом при передаче такой информации нода-мастер, с которой происходит передача информации на реплику блокируется на какое то время для того чтобы получить сигнал о том, что эта нода либо приняла, либо приняла и применила на уровне ОС, либо приняла и применила на уровнях ОС и SQL этот запрос. Пока такой сигнал не придет обратно на мастер-ноду, она не производит дополнительных штук.

Также у нас есть такое понятие как лаг-репликация, то есть отставание.

![sync](images/14_01.png) ![async](images/14_02.png)

Репликация - это что то, что должно идти обязательно до бэкапирования! То есть в целом для построения нашей большой системы - это построение кластера самой субд, мониторинг, алертинг, репликация, бэкапирование. Бэкапирование - это что то, до чего мы не должны дойти, тк у нас есть репликация и она должна быть настроена!

Типы передаваемой информации в рамках репликации:
- Statement - Реплицируются запросы на выполнение; данные есть, мы говорим, что нужно сделать над этими данными чтобы привести их в то же самое состояние
- Row - реплицируются сами данные и изменения этих данных (некоторый блок измененных строк, который мы можем передавать от мастера к реплике для того, чтобы реплика в таком вот смысле приводила себя к общему состоянию)
- Mixed - совмещение подходов (передаем и сами запросы на выполнение, и сами измененные строки)

![statement, row, mixed](images/14_03.png)

В клике используется асинхронный мульти-лидер. Каждая реплика самостоятельно принимает данные
- данные о новых партах записываются в метаданные в zookeeper/clickhouse-keeper (далее *keeper), и в очередь *keeper как событие GET_PART ; Зукипер - дополнительный инструмент, необходимый нам в клике для репликации. Конкретик по версии зукипера нет, все удобноваримые. Кликхаускипер - тот же самый зукипер, но разработанный на плюсах и оптимизированный специально для кликхауса. У зукипера есть буквально две хорошие версии - 3.5.4 и 3.7.х .  
  Также в зукипер записывается, какое событие именно произошло. GET_PART - это самое частое используемые. То есть те реплики, которые подключены к зукиперу (то есть находятся в одной реплике по шардам ) будут знать, что, допустим, первая реплика как лидер отправила запрос что произошло изменение наших парт и нам нужно новую измененную парту здесь изобразить. Понимает он это как раз по очереди GET_PART. Есть еще 2 события (которые отвечают за слияние парта и за операцию связанную с exchange или с работу с большой партицией, но они достаточно редки)
- каждая реплика слушает http-порт заданный в конфигурации как interserver_http_port И/ИЛИ interserver_https_port, а так же анонсирует себя в *keeper. Эту информацию можно посмотреть в наших системных представлениях
- каждая реплика разбирает очередь и выполняет GET_PART, скачивая с соседних реплик недостающие данные

Архитектурно исключены проблемы с асинхронной репликацией
- нет требований к уникальности Primary Key
- нет автоинкремента

Кластеры зукипера могут быть в зависимости от размера кластера/шарда разные - может быть один отдельный большой кластер зукипера (есть кластер клика на энное количество нод, а рядом с ним кластенр зукипера), а бывате так что нагрузка на передачу метаинформации между нодами достаточно высокая и отдельные шарды (достаточно жирные) будут перекрывать полностью загруженность зукипера, в таком случае настраивается кластер зукипера для отдельных шардов клика. Как правило, кластер зукипера - это 3 ноды. При этом даже если клик является нодами железными, которые расположены на железе, то ресурсы для зукипера не должны быть высокими! Поэтому их можно делать на виртуалках, железки обычно на них не заказываются! 

На каждую из групп шардов может быть свой кластер зукипера (слева шарды, справа зукиперы):

![ds](images/14_04.png)

Но тут может быть следующее неудобство для работы - запрос на класетр. В таком случае мы не можем создавать удобную `create table on cluster` - в такой конфигурации нам требуется один единый зукипер на весь кликхаус. Из за этого могут быть ошибки, так что лучше такую возможность заблокировать (чтобы on cluster мы не могли делать)!

В рамках домашки можно развернуть кластер кликхауса в докере - там 3 ноды кликхауса и 1 нода зукипера. [Докер компоуз](https://chistadata.com/data-replication-in-clickhouse-docker-based-setup/). Клики в докер компоузе абсолютно одинаковые. Надо зайти на каждую ноду чтобы произвести опереденные настойки. НО!!!! конфиг.xml не менять! (в инструкции он меняется).

```sql
show clusters;  -- смотрим настроенные кластеры, где и производится репликация
select * from system.replicas \G;  -- здесь можем посмотреть информацию по текущей реплике, которая была создана
/*
Важные поля:
is_leader - нода, с которой я в текущий момент произвожу вставку данных и с которой переходит информация по всем остальным нодам (get_part)
can_become_leader - можем и указать 0, допустим если у нас нода, которую мы отцепили для какого нибудь бэкапирования.
is_readonly - 1 когда что то пошло не так
zookeeper_exception - а тут пишется, что именно пошло не так

если какие то специальные параметры не зададим - is_leader и can_become_leader будут единички
*/
```
Почему мы не боимся репликации мастер-мастер (то есть мультилидер). Во первых у нас нет требования к уникальности первичного ключа. Также нет автоинкремента - при передаче метаинформации в случае если мы произвели какую то операцию - об этой информации появилась строчка в метаданных и в обычных системах еще появляется строчка с счетчиком (инкрементом). При одновременных вставках может произойти наложение автоинкремента, поскольку каждая из наших нод будет видеть свою часть. Из-за этого могут быть как дубли, так и потери данных.

- Ближайший аналог для репликации данных - ROW. Репликация данных происходит партами, их можно считать блоками строк, но правильнее будет их называть блоками колонок, а репликацию PART-Based. Кроме результатов фоновой операции MERGE_PARTS, можно однако настроить и эти данные перевыкачиваться готовыми с реплик, а не пересчитываться. (то есть может быть настройка перекачивать готовыми, а может быть настройка - перекидывать что нужно сделать). Еще один момент - поскольку данные выкачиваются блоками парт, парты у нас заканчиваются и нам нужно как то пометить те парты, которые у нас не используются. Собственно, они помечаются , в процессе произведения последних вычислений они отсеиваются по определенному флагу. Фоновая операция MERGE_PARTS как раз за это и отвечает - что старые данные не переливаются.
- Для репликации метаданных - Statement. 
- Для репликации мутаций - Statement.

Именно поэтому у нас mixed - часть операций производится с помощью дифов (изменений данных, того, что нам нужно сделать на наших репликах)

Требования к репликации:
1. работающий зукипер
2. секция в конфигурации, смотрящая на живой зукипер (<zookeper>...</zookeper>)
3. Replicated к движку таблицы
4. Дополнительные параметры для движка
   - path - путь в *keeper (путь, по которому зукипер и другие реплики будут понимать, где что лежит)
   - replica - уникальное имя текущей реплики
5. уникальный путь в зукипере

То есть нам нужно 3 секции:
- remote_servers - отвечает за наименование и настройки нашего кластера (у нас есть нода 1, 2, 3 - как именно они должны между собой связываться)
- replicated_cluster
- macros - необязательный, но достаточно часто и успешно используется

Макросы - применяются для упрощения задания параметров репликации. Конфигурация
```xml
<macros>
    <macros_name>macros_value</macros_name>
</macros>
```
Наиболее часто используемые <shard> и <replica> либо <host>.

Встроенные макросы, которые не нужно дополнительно определять
- <database> - подставит имя текущей базы
- <table> - создаваемой таблицы

Пример использования макросов:
```sql
CREATE TABLE .... 
ENGINE=Replicated...(‘/clickhouse/shard_{shard}/{database}/{table}’,’{replica}’, ...)
```

Пример конфигурации
```xml
<zookeeper>
    <node index="1">
        <host>zk-server1.zone</host>
        <port>2181</port>
    </node>
    <node index="2">
        <host>zk-server2.zone</host>
        <port>2181</port>
    </node>
    <node index="3">
        <host>zk-server3.zone</host>
        <port>2181</port>
    </node>
</zookeeper>
```
Индексы могут указываться, но чисто для клика они никакой смысловой нагрузки не несут - просто для удобства использования. Можно 6, 7, 8 сделать - клик ругаться не будет

Пример таблицы
```sql
CREATE TABLE replication_test
(
    `a` String
)
ENGINE = ReplicatedMergeTree('/some_uniq_zookeeper_path', 'some_replica_name')
ORDER BY a
```

## Практика 42 00

```bash
# заходим на каждый из контейнеров кликхауса
apt-get update
apt-get install vim

vi /etc/clickhouse-server/config.d/z_config.xml
# и вставляем туда конфиг из cluster_ch/cluster_config.xml
:q

# про internal_replication и shard - будет на след лекции

clickhouse-client
# проверяем что все ок
show clusters  # если видется наш кластер - значит все ок
# /*
#    ┌─cluster────────────┐
# 1. │ default            │
# 2. │ replicated_cluster │
#    └────────────────────┘

# 2 rows in set. Elapsed: 0.005 sec. 
# */
```

Попробуем сделать таблицы:
```sql
create table example on cluster replicated_cluster
(
    id UInt32,
    name String
)
engine=MergeTree()
order by id;
-- on cluster replicated_cluster - вспомогательная удобная штука. Если у нас один и тот же зукипер - нам не придется подключаться на каждую ноду для создания таблицы. Может быть указан не только для ддл - а еще и для создания ролей и тд
-- on cluster - только чтобы создать ддл
-- ReplicatedMergeTree - чтобы скопировать данные

create table replicated_example
(
    id UInt32,
    name String
)
engine=ReplicatedMergeTree('/clickhouse/{table}', '{replica}')
order by id;
-- replica, table достается из макроса
-- /clickhouse - иногда есть рекомендация прописывать путь начиная от /clickhouse когда зукипер используется еще для каких то нужд (сервисов), чтобы в случае если нам нужно раскопать, что пошло не так - мы могли бы сразу понять где метаинформация, которая относится к клику

insert into replicated_example values (1, 'aaa');
-- данные появятся на всех нодах
```

Добавление реплики:
- разворачивается ещё один сервер вашей системой управления конфигурациями
- идентичная конфигурация ClickHouse, за минусом секции <macros> (поскольку должна быть уникальной) и параметров идентифицирующих сервер, таких как <inserserver_host>, если вас не устроилизначенияпоумолчанию, в противном случае только <macros>. 
- создаются все таблицы как на соседней реплике, с такими же path в *keeper, с отличным значением параметра <replica>. 
- дальше ClickHouse самостоятельно выкачает по репликации все данные

Конвертация таблицы в реплицируемую: [ссылка](https://clickhouse.com/docs/en/sql-reference/statements/alter/partition), Еще  одна полезная [ссылка](https://habr.com/ru/companies/digitalleague/articles/759316/) . Есть много различных способов, вот один из них
- создаем новую таблицу `CREATE TABLE new AS old ENGINE=Replicated*(2 параметра реплиации, ...)` остальное также как в оригинальной таблице
- выполняем `ALTER TABLE new ATTACH PARTITION ID ‘...’ FROM old` для всех партиций старой таблицы, можно их найти как `SELECT DISTINCT partition_id FROM system.parts where database || ‘.’ || table == ‘database.table’;`
- `EXCHANGE TABLES new AND old; DROP TABLE new;` или `DROP TABLE old; RENAME TABLE new TO old;` или просто удаляем старую если новое имя нас устраивает.

Зачем нужен leader в ClickHouse? Это не такой лидер как в постгресе - это скорее номинальное назначение для какой то ноды, что "ты - молодец, ты сейчас рулишь процессом". 
- назначает операции MERGE_PARTS
- можно крутить ручкой can_become_leader
- может быть несколько лидеров или все реплики лидерами, в зависимости от версии ClickHouse

Скорость репликации - Она высокая, никакого пережатия и дообработки данных не происходит, только подсчет контрольных сумм, узкое место - скорость дисковой системы, иногда *keeper если он перегружен или данные поступают часто мелкими батчами, накапливая большое количество партов. Если *keeper в порядке, а дисковая подсистема позволяет, можно утилизировать любой сетевой интерфейс. Например, забить аплинк между датацентрами, одновременно поставив заливаться десяток-другой реплик с 25Gbit интерфейсами. Ограничивать скорость можно и нужно параметрами: 
- max_replicated_sends_network_bandwidth_for_server (предпочтительно) (нагрузка на конкретную сетевую инфраструктуру) или
- max_replicated_fetches_network_bandwidth_for_server (тоже можно) (то есть на отправку и на забор - но это тонкости, когда работаем с большими объемами данных. Часто они остаются по умолчанию)

Состояние репликации: 

`system.replicas` - системная таблица с состоянием репликации каждой таблицы. полезные поля: 
- is_readonly - 0 когда всё хорошо, 1 когда репликация не работает, таблица блокируется назапись
- last_queue_update_exception и zookeeper_exception - ошибка, смотреть если is_readonly
- absolute_delay / queue_size / {inserts,merges,part_mutations}_in_queue - лаг репликациии состояние очередей. queue_size - размер лага в общем количестве операций
- {total,active}_replicas / replica_is_active - сколько всего и где реплики есть у таблицы

`system.replication_queue` - состояние очереди репликации, полезные поля: 
- type - тип события репликации: GET_PART, REPLACE_RANGE, MERGE_PARTS
- postpone_reason - обычно это «Not executing fetch ... because 8 fetches already executing» и это нормально
- num_postponed - сколько раз откладывалось
- num_tries - попыток; last_exception - почему не получилось, например не хватило диска/памяти, сетевые обрывы  
В таблице отображается именно очередь! если таблица пустая - это абсолютно нормально


## TTL  (1 22 00)

ТТЛ - отложенное удаление данных. Главный кейс - удаление

По сути ТТЛ - это обычный крон с `alter table drop partition` / `alter table move partition`

Объявляется для столбца - `CREATE TABLE table ( ... column ТИП TTL timestamp + INTERVAL 1 hour ... )`  
Или для таблицы `CREATE TABLE table ( ... ) ... TTL timestamp + INTERVAL 1 hour`  
- `timestamp` - любая колонка Date или DateTime
- `INTERVAL 1 hour` - описание специального типа interval

- для столбцов удаляет значение столбца когда `now() >= timestamp + интервал`
- для таблицы - целиком строку

Удаление здесь происходит не через мутацию - просто наши парты не сливаются

При наличии storage_policy, можно вместо удаления перещать данные между дисками, синтаксис: `TTL timestamp + INTERVAL 1 hour TO VOLUME вольюм_из_конфигурации` , объявляется на таблицу

альтернатива. В целом, ттл - не самый надежный способ для использования, есть более надежный и дешевый способ. Важно понимать, что сравнение происходит во время Merge партов, и стоит ресурса в размере сравнения timestamp с каждой строкой, на нагруженной системе Merge парта может случиться сильно позже объявленного TTL или не случиться вовсе при достижении макс. размера парта. Есть ручка, меняющая это поведение на близкое к крону дропающему партиции - ttl_only_drop_parts  
Более надежным/эффективным/дешевым способом является простой cron c `ALTER TABLE DROP PARTITION` или `ALTER TABLE MOVE PARTITION` (но это если есть доступ для крона)

Такие вещи (чтобы ттл работало некорректно) достаточно редки, но все таки могут быть

```bash
0 0 * * * clickhouse-client < 1.sql  # пример крона
```

## Мутации (1 30 00)

Виды и применимость мутаций в теме «Мутация данных и манипуляции с партициями» курса.Мутации так же являются фоновыми операциями и статус их выполнения можно наблюдать в таблице system.mutations, важные колонки: 
- command - запрос описывающий мутацию
- is_done - становится =1 когда успешно выполнена
- latest_fail_reason - последняя ошибка
- parts_to_do_names / parts_to_do - прогресс выполнения

есть системная таблица-представление, которая указывает текущии мутации, которые бегут. 

## Merge операции

Когда иное не подразумевается контекстом, или type операции Merge не является чем то отличным от MERGE_PARTS, под Merge понимают объединение партов (в фоне, по умолчаниб раз в 8 минут, но зависит от загруженности - на это нельзя рассчитывать). Есть 2 алгоритма объединения, Horizontal и Vertical. 
- Horizontal - по умолчанию, выбирает столько, сколько удовлетворяет настройке max_bytes_to_merge_at_max_space_in_pool (150Гб default). То есть это не одна парта, которая столько весит - а парт набрать на столько, сколько здесь указано. Все эти парты будут сливаться в ходе одного фонового процесса.
- Vertical - выбирает строго 2 парта для объединения, включается автоматически при достижении одного из критериев: 
  - vertical_merge_algorithm_min_rows_to_activate = 131072
  - vertical_merge_algorithm_min_bytes_to_activate = inf по умолчанию
  - vertical_merge_algorithm_min_columns_to_activate = 11 столбцов не-ключа  
  неэффективно на больших объемах, поскольку затрачивается больше ресурсов. Но если мы забрали из кафки или из апи какие то жирные джсоны - то нам будет удобней применить вертикальное слияние.

Кроме MERGE_PARTS есть типы Merge-операций: 
- MUTATE_PART (отображается как пустая строка в system.merges) для мутаций 
- REPLACE_RANGE для ALTER TABLE REPLACE PARTITION
- TTL

## Выделение ресурсов под фоновые операции

Количество одновременных фоновых задач репликации, Merge партов и TTL определяется пулами тредов, а именно:
- background_pool_size (default: 16, в старых версиях 8) - количество тредов для операций Merge и мутаций
- background_move_pool_size (default: 8 ) - пул для Merge-операций типа MOVE_PART
- background_fetches_pool_size (default: 16) - пул для GET_PART

В старых версиях выставляется пользователю default, в новых в основной конфиг сервера. Список и назначение пулов меняется от версии к версии, актуальный можно получить в метриках запросом 
```sql
SELECT *
FROM system.metrics
WHERE metric LIKE 'Background%' ;
/*
Row 1:
──────
metric:      BackgroundMergesAndMutationsPoolTask
value:       0
description: Number of active merges and mutations in an associated background pool

Row 2:
──────
metric:      BackgroundMergesAndMutationsPoolSize
value:       64
description: Limit on number of active merges and mutations in an associated background pool

Row 3:
──────
metric:      BackgroundFetchesPoolTask
value:       0
description: Number of active fetches in an associated background pool

Row 4:
──────
metric:      BackgroundFetchesPoolSize
value:       32
description: Limit on number of simultaneous fetches in an associated background pool

Row 5:
──────
metric:      BackgroundCommonPoolTask
value:       0
description: Number of active tasks in an associated background pool

Row 6:
──────
metric:      BackgroundCommonPoolSize
value:       16
description: Limit on number of tasks in an associated background pool

Row 7:
──────
metric:      BackgroundMovePoolTask
value:       0
description: Number of active tasks in BackgroundProcessingPool for moves

Row 8:
──────
metric:      BackgroundMovePoolSize
value:       16
description: Limit on number of tasks in BackgroundProcessingPool for moves

Row 9:
───────
metric:      BackgroundSchedulePoolTask
value:       0
description: Number of active tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc.

Row 10:
───────
metric:      BackgroundSchedulePoolSize
value:       512
description: Limit on number of tasks in BackgroundSchedulePool. This pool is used for periodic ReplicatedMergeTree tasks, like cleaning old data parts, altering data parts, replica re-initialization, etc.

Row 11:
───────
metric:      BackgroundBufferFlushSchedulePoolTask
value:       0
description: Number of active tasks in BackgroundBufferFlushSchedulePool. This pool is used for periodic Buffer flushes

Row 12:
───────
metric:      BackgroundBufferFlushSchedulePoolSize
value:       16
description: Limit on number of tasks in BackgroundBufferFlushSchedulePool

Row 13:
───────
metric:      BackgroundDistributedSchedulePoolTask
value:       0
description: Number of active tasks in BackgroundDistributedSchedulePool. This pool is used for distributed sends that is done in background.

Row 14:
───────
metric:      BackgroundDistributedSchedulePoolSize
value:       16
description: Limit on number of tasks in BackgroundDistributedSchedulePool

Row 15:
───────
metric:      BackgroundMessageBrokerSchedulePoolTask
value:       0
description: Number of active tasks in BackgroundProcessingPool for message streaming

Row 16:
───────
metric:      BackgroundMessageBrokerSchedulePoolSize
value:       16
description: Limit on number of tasks in BackgroundProcessingPool for message streaming

16 rows in set. Elapsed: 0.009 sec. 
*/

-- чаще всего страдают (мы отключаем) TTL-операции - поскльку они самые ресурсоемкие
```

